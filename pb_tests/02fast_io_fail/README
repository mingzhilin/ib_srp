TEST: 02fast_io_fail
====================

Copyright ProfitBricks GmbH - All rights reserved

Author:  Sebastian Riemer <sebastian.riemer@profitbricks.com>
License: GPLv2

Description
-----------

The goal of this test is to check if "fast IO failing" works
correctly. We register for that on the blk and the QP timeout
and see what triggers first. The QP timeout is at approx. 35s
by default with the default 7 QP retries.

So we have to check three cases here:

1. Check if mainline behavior still works (no fast IO failing)
2. Check if block layer timeout works (timeout: 15s)
3. Check if QP timeout works (blk timeout > QP timeout: 40s)

Required Hardware
-----------------

You should have a target system and an initiator system connected
with two IB/RDMA paths. The smallest possible test setup is
composed of two desktop machines with two IB/RDMA ports each
connected back2back.

Preparation
-----------

Check for all shell scripts in 'common' and this directory if
the configuration is set correctly for your setup. You should
run SCST on the target and the ib_srp driver from this repo
on the initiator.

Make sure you've got some equal sized files prepared - all
with the same prefix and a number at the end starting at '1'
(e.g. "vol1", "vol2", ...).

ATTENTION: The multipath-tools from the following location
must be installed on the initiator side:

https://github.com/sriemer/multipath-tools

An important patch is included there to fix a major bug to
prevent IO on offline devices.

Patch: "libmultipath/discovery: read sysfs files uncached"

Why it hasn't been accepted upstream:
http://comments.gmane.org/gmane.linux.kernel.device-mapper.devel/18188

We use the default multipath config which means that the
directio path checker is used. It reads the first sector
of a SCSI disk every 5s.

============================================================

Step 1: Preparation
-------------------

Target:
$ ./start_scst.sh

Initiator:
$ /etc/init.d/multipath-tools start
$ ./connect_srp.sh
$ multipath -ll

Expected result:
The multipath-tools are running.
Target and initiator are connected via SRP, the exported
LUNs are available at both paths on the initiator. All
SRP SCSI disks have the same default block layer timeout.
This should be '61' (fast IO failing disabled) by default.

There is a device mapper device per LUN with two paths
each.

Step 2: Do and monitor IO
-------------------------

Pick a device mapper device you want to do IO on. We will
read from it continuously. Here we assume '/dev/dm-0' and
that '/dev/sdb' and '/dev/sde' belong to that dm-multipath
device.

Initiator:
$ while [ 1 ]; do dd if=/dev/dm-0 iflag=direct bs=1M of=/dev/null; done
# <switch terminal>
$ iostat -x -m sdb sde 1
# <switch terminal>

Expected result:
You should see a high IO rate on one of the component
devices and the other is absolutely idle.

Step 3: Let IO fail
-------------------

There are two possibilities to let the IO fail with time
relation:

1. Pull the cable where the IO flows.
2. At least with IB we can hold down a port with
   'ibportstate'. Perhaps, with RoCE or iWARP 'ifdown'
   and later 'ifup' works.

Initiator:
# <look at your watch for the start time>
# <Hold down the port for 90s :>
$ ./turnoff_ib.sh 90
# <look the iostat>
# <wait for IO failing over>
# <check the elapsed time>
$ dmesg | less

Expected result:
It takes between 1:00 min and 1:15 min until you can
see that IO is failed over to the other path.
The disrupted SRP host is NOT removed in contrast to
the actual mainline behavior. This allows repeated
reconnects.

Kernel log: IO is failed with SCSI debugging output.
The SCSI result is "DID_ABORT". You can see lots of
calls to srp_abort(), srp_reset_device() and a call
to srp_reset_host(). The reconnect wasn't possible
as we've helt down the SRP connection for longer
than the block layer timeout (61s). The SCSI error
handling offlines the devices. You can't see IO
to offline devices by the multipath-tools.

Step 4: Redo with blk timeout
-----------------------------

Initiator:
# <go to 'dd' terminal, stop IO>
# <stop iostat>
$ ./remove_all_paths.sh
$ ./disconnect_srp.sh
$ ./connect_srp.sh
$ ./set_timeout.sh 15
$ multipath -ll
# <repeat steps 2 and 3>
$ cat /sys/block/sd[b-z]/device/state
$ cat /sys/class/srp_host/*/device/scsi_host/*/state

Expected result:
IO is failed in exactly the time of the timeout or
max. 3s later. This is here 15..18s. There is an
initial latency until you see in iostat that the
the IO rate is at 0 MB/s and the port is down.
This explains possible time differences.

In the kernel log you can see that there are only
srp_abort() calls triggered at the same time. IO
is failed silently (no SCSI debugging in the kernel
log). No IO to offline devices.

The SCSI device state is at 'transport-offline'
for this path. The SCSI host state is still at
'running' for this path.

Step 5: Redo with QP timeout
----------------------------

# <redo step 4 but use 40s as timeout :>
$ ./set_timeout.sh 40

Expected result:
Like in step 4 but it takes only 35s to fail the
path instead of the set 40s. You'll see in the
kernel log that the order of the messages is
different.

Before:
scsi host6: ib_srp: srp_tl_err_work called
scsi host6: ib_srp: failed send status 5

Now:
scsi host6: ib_srp: failed send status 5
scsi host6: ib_srp: srp_tl_err_work called

It is likely that there are no srp_abort()
calls. But if there are any, it is also
okay.

Step 6: Roll back for the next test
-----------------------------------

Initiator:
# <go to 'dd' terminal, stop IO>
# <stop iostat>
$ ./remove_all_paths.sh
$ ./disconnect_srp.sh

Target:
$ ./stop_scst.sh

Expected result:
No SRP initiator or target modules are loaded.
